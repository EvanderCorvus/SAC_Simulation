{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First RL Gym Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from environments import BoxEnvironment1 as env\n",
    "from environment_utils import Box\n",
    "from agents import SACAgent\n",
    "from agent_utils import update_target_agent, ReplayBuffer\n",
    "from log_utils import RLLogger\n",
    "from plot_utils import RLPlotter, plot_normalized_mexican_hat_potential\n",
    "\n",
    "device = tr.device('cuda' if tr.cuda.is_available() else 'cpu')\n",
    "tr.autograd.set_detect_anomaly(True)\n",
    "tr.set_default_tensor_type(tr.FloatTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Training -------------------\n",
    "    # Memory\n",
    "memory_size = 3000\n",
    "memory_batch_size = 512\n",
    "    # Duration of training\n",
    "runs = 1\n",
    "n_episodes = 50\n",
    "n_steps = 256\n",
    "    # Training parameters\n",
    "agent_batch_size = 128\n",
    "learning_rate_actor = 0.001\n",
    "learning_rate_critic = 0.001\n",
    "entropy_coef = 0.02\n",
    "    # Bellman equation\n",
    "future_discount = 0.999\n",
    "    # Update Target Model\n",
    "target_model_update = 2\n",
    "    # Loss Function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# ---------------- Environment  ----------------\n",
    "    # Environment box size\n",
    "env_width = 2\n",
    "env_height = 2\n",
    "space = Box(env_width, env_height)\n",
    "    # Goal box size and center\n",
    "goal_width = 0.3\n",
    "goal_height = 0.3\n",
    "goal_center = np.tile([0.5,0],(agent_batch_size,1))\n",
    "goal = Box(goal_width, goal_height, goal_center)\n",
    "    # Time step size\n",
    "dt = 0.04\n",
    "    # Noise\n",
    "noise_characteristic_length = 4\n",
    "    # Maximum of potential\n",
    "U0 = 0.5\n",
    "\n",
    "# ---------------- Agent ----------------------\n",
    "state_dim = 4\n",
    "hidden_dims = [16,16]\n",
    "act_dim = 1\n",
    "act_positive = True\n",
    "act_scaling = 2*np.pi\n",
    "\n",
    "# ---------------- Other ----------------------\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "plt.rcParams.update({'figure.dpi': 150})\n",
    "total_time = []\n",
    "update_state_time = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = env(space, goal)\n",
    "memory = ReplayBuffer(state_dim, act_dim, memory_size, agent_batch_size)\n",
    "agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "target_agent = SACAgent(state_dim, act_dim, hidden_dims, act_scaling, act_positive).float().to(device)\n",
    "logger = RLLogger()\n",
    "plotter = RLPlotter(logger, goal, 'logs')\n",
    "testLogger = RLLogger()\n",
    "testPlotter = RLPlotter(testLogger, goal, 'test_logs', test=True)\n",
    "\n",
    "agent.actor_optimizer = tr.optim.Adam(agent.actor.parameters(), lr=learning_rate_actor)\n",
    "agent.critic1_optimizer = tr.optim.Adam(agent.critic1.parameters(), lr=learning_rate_critic)\n",
    "agent.critic2_optimizer = tr.optim.Adam(agent.critic2.parameters(), lr=learning_rate_critic)\n",
    "\n",
    "for p in target_agent.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(agent, target_agent, memory_batch):\n",
    "    agent.critic1_optimizer.zero_grad()\n",
    "    agent.critic2_optimizer.zero_grad()\n",
    "\n",
    "    state_now = memory_batch['state_now'].reshape(-1, state_dim)\n",
    "    state_next = memory_batch['state_next'].reshape(-1, state_dim)\n",
    "    action_now = memory_batch['action_now'].reshape(-1, act_dim)\n",
    "    reward = memory_batch['reward'].reshape(-1)\n",
    "    done = memory_batch['done'].reshape(-1)\n",
    "    \n",
    "    # Compute Prediction\n",
    "    Q1_now_critic = agent.critic1(state_now, action_now)\n",
    "    Q2_now_critic = agent.critic2(state_now, action_now)\n",
    "\n",
    "    # Compute Target\n",
    "    with tr.no_grad():        \n",
    "        action_next_critic, log_prob_next_critic = agent.actor(state_next)\n",
    "        \n",
    "        Q1_next_critic = target_agent.critic1(state_next, action_next_critic)\n",
    "        Q2_next_critic = target_agent.critic2(state_next, action_next_critic)\n",
    "        Q_next_critic = tr.min(Q1_next_critic, Q2_next_critic)\n",
    "        target_critic = reward + future_discount*(Q_next_critic - entropy_coef*log_prob_next_critic)\n",
    "    # Compute Loss\n",
    "    loss_critic = loss_function(Q1_now_critic, target_critic) + loss_function(Q2_now_critic, target_critic)\n",
    "    \n",
    "    # Update\n",
    "    loss_critic.backward()\n",
    "    agent.critic1_optimizer.step()\n",
    "    agent.critic2_optimizer.step()\n",
    "    \n",
    "    agent.actor_optimizer.zero_grad()\n",
    "    for p in agent.critic1.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in agent.critic2.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    action_now_actor, log_prob_now_actor = agent.actor(state_now)\n",
    "    Q1_now_actor = agent.critic1(state_now, action_now_actor)\n",
    "    Q2_now_actor = agent.critic2(state_now, action_now_actor)\n",
    "    Q_now_actor = tr.min(Q1_now_actor, Q2_now_actor)\n",
    "    loss_actor = (entropy_coef*log_prob_now_actor - Q_now_actor).mean()\n",
    "    loss_actor.backward()\n",
    "    agent.actor_optimizer.step()\n",
    "\n",
    "    for p in agent.critic1.parameters():\n",
    "        p.requires_grad = True\n",
    "    for p in agent.critic2.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return loss_critic, loss_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode():    \n",
    "    environment.init_state(agent_batch_size, state_dim)\n",
    "    if memory.size < memory_batch_size:\n",
    "        dt = 0.08\n",
    "    else:\n",
    "        dt = 0.04\n",
    "    for current_step in range(n_steps):\n",
    "        # Log state\n",
    "        logger.save_state(environment.state)\n",
    "        if current_step%target_model_update == 0 and current_step > memory_size:\n",
    "            update_target_agent(agent, target_agent)\n",
    "        # Beginning state\n",
    "        state_now = environment.state\n",
    "        # Action\n",
    "        if memory.size < memory_batch_size:\n",
    "            action_now = 2*tr.pi*tr.rand(agent_batch_size, act_dim, device=device, dtype=tr.float)\n",
    "        else:\n",
    "            action_now, _ = agent.actor(tr.as_tensor(environment.state, device=device, dtype=tr.float))\n",
    "        # Next state\n",
    "        reward = environment.step(action_now.detach().cpu().numpy(), U0, dt, noise_characteristic_length)\n",
    "        state_next = environment.state\n",
    "        # Done\n",
    "        done = environment.goal_check()\n",
    "        # Log action\n",
    "        logger.save_action(action_now.detach().cpu().numpy())\n",
    "\n",
    "        loss = 0\n",
    "        # Sample from memory\n",
    "        if memory.size >= memory_batch_size:\n",
    "            memory_batch = memory.sample_batch(memory_batch_size)\n",
    "            # Update Agent\n",
    "            loss_critic, loss_actor = update(agent, target_agent, memory_batch)\n",
    "            loss_critic, loss_actor = loss_critic.item(), loss_actor.item()\n",
    "            logger.save_loss_critic(loss_critic)\n",
    "            logger.save_loss_actor(loss_actor)\n",
    "        \n",
    "        # Store in memory\n",
    "        memory.store(state_now, action_now, reward, state_next, loss, done)\n",
    "\n",
    "        if max(environment.goal_check()): \n",
    "            print('Goal reached')\n",
    "            logger.save_state(environment.state)\n",
    "            break\n",
    "        \n",
    "    return current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode():\n",
    "    environment.init_state(1, state_dim)\n",
    "    for current_step in range(n_steps):\n",
    "        # Log state\n",
    "        testLogger.save_state(environment.state)\n",
    "        # Action\n",
    "        action_now = agent.act(tr.as_tensor(environment.state, device=device, dtype=tr.float),deterministic=True)\n",
    "        environment.step(action_now, U0, dt, noise_characteristic_length, True)\n",
    "\n",
    "        # Log action\n",
    "        testLogger.save_action(action_now)\n",
    "\n",
    "        if max(environment.goal_check()): \n",
    "            print('Goal reached')\n",
    "            testLogger.save_state(environment.state)\n",
    "            break\n",
    "    return current_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached\n",
      "Episode 0  finished!\n",
      "Goal reached\n",
      "Episode 1  finished!\n",
      "Goal reached\n",
      "Episode 2  finished!\n",
      "Goal reached\n",
      "Episode 3  finished!\n",
      "Goal reached\n",
      "Episode 4  finished!\n",
      "Episode 5  finished!\n",
      "Goal reached\n",
      "Episode 6  finished!\n",
      "Episode 7  finished!\n",
      "Episode 8  finished!\n",
      "Episode 9  finished!\n",
      "Goal reached\n",
      "Goal reached\n",
      "Episode 10  finished!\n",
      "Goal reached\n",
      "Goal reached\n",
      "Episode 11  finished!\n",
      "Goal reached\n",
      "Goal reached\n",
      "Episode 12  finished!\n",
      "Goal reached\n",
      "Goal reached\n",
      "Episode 13  finished!\n",
      "Episode 14  finished!\n",
      "Goal reached\n",
      "Episode 15  finished!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m plotter\u001b[39m.\u001b[39mclear_plots(\u001b[39m'\u001b[39m\u001b[39mlogs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m testPlotter\u001b[39m.\u001b[39mclear_plots(\u001b[39m'\u001b[39m\u001b[39mtest_logs\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m simulation()\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36msimulation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m update_target_agent(agent, target_agent)\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n\u001b[1;32m----> 4\u001b[0m     episode_steps \u001b[39m=\u001b[39m episode()\n\u001b[0;32m      5\u001b[0m     logger\u001b[39m.\u001b[39msave_episode(episode_steps)\n\u001b[0;32m      6\u001b[0m     plotter\u001b[39m.\u001b[39mplot_last_episode()\n",
      "Cell \u001b[1;32mIn[5], line 32\u001b[0m, in \u001b[0;36mepisode\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m memory_batch \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39msample_batch(memory_batch_size)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Update Agent\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m loss_critic, loss_actor \u001b[39m=\u001b[39m update(agent, target_agent, memory_batch)\n\u001b[0;32m     33\u001b[0m loss_critic, loss_actor \u001b[39m=\u001b[39m loss_critic\u001b[39m.\u001b[39mitem(), loss_actor\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     34\u001b[0m logger\u001b[39m.\u001b[39msave_loss_critic(loss_critic)\n",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(agent, target_agent, memory_batch)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m agent\u001b[39m.\u001b[39mcritic2\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m     35\u001b[0m     p\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m action_now_actor, log_prob_now_actor \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mactor(state_now)\n\u001b[0;32m     38\u001b[0m Q1_now_actor \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mcritic1(state_now, action_now_actor)\n\u001b[0;32m     39\u001b[0m Q2_now_actor \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mcritic2(state_now, action_now_actor)\n",
      "File \u001b[1;32mc:\\Users\\igonn\\anaconda3\\envs\\tr\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\igonn\\Desktop\\My Projects\\SAC_Simulation\\agents.py:45\u001b[0m, in \u001b[0;36mSquashedGaussianActor.forward\u001b[1;34m(self, state, deterministic, with_logprob)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_positive:\n\u001b[0;32m     43\u001b[0m     pi_action \u001b[39m=\u001b[39m (pi_action \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m---> 45\u001b[0m pi_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact_scaling \u001b[39m*\u001b[39;49m pi_action\n\u001b[0;32m     47\u001b[0m \u001b[39mif\u001b[39;00m tr\u001b[39m.\u001b[39misnan(pi_action)\u001b[39m.\u001b[39many():\n\u001b[0;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpiaction is nana\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\igonn\\anaconda3\\envs\\tr\\Lib\\site-packages\\torch\\fx\\traceback.py:41\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m [current_meta\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstack_trace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[39m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m traceback\u001b[39m.\u001b[39mformat_list(traceback\u001b[39m.\u001b[39;49mextract_stack()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\igonn\\anaconda3\\envs\\tr\\Lib\\traceback.py:231\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[1;32m--> 231\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m    232\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[0;32m    233\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\igonn\\anaconda3\\envs\\tr\\Lib\\traceback.py:393\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[39mfor\u001b[39;00m f, lineno \u001b[39min\u001b[39;00m frame_gen:\n\u001b[0;32m    391\u001b[0m         \u001b[39myield\u001b[39;00m f, (lineno, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 393\u001b[0m \u001b[39mreturn\u001b[39;00m klass\u001b[39m.\u001b[39;49m_extract_from_extended_frame_gen(\n\u001b[0;32m    394\u001b[0m     extended_frame_gen(), limit\u001b[39m=\u001b[39;49mlimit, lookup_lines\u001b[39m=\u001b[39;49mlookup_lines,\n\u001b[0;32m    395\u001b[0m     capture_locals\u001b[39m=\u001b[39;49mcapture_locals)\n",
      "File \u001b[1;32mc:\\Users\\igonn\\anaconda3\\envs\\tr\\Lib\\traceback.py:432\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    428\u001b[0m     result\u001b[39m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    429\u001b[0m         filename, lineno, name, lookup_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39mf_locals,\n\u001b[0;32m    430\u001b[0m         end_lineno\u001b[39m=\u001b[39mend_lineno, colno\u001b[39m=\u001b[39mcolno, end_colno\u001b[39m=\u001b[39mend_colno))\n\u001b[0;32m    431\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m fnames:\n\u001b[1;32m--> 432\u001b[0m     linecache\u001b[39m.\u001b[39;49mcheckcache(filename)\n\u001b[0;32m    433\u001b[0m \u001b[39m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mc:\\Users\\igonn\\anaconda3\\envs\\tr\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m   \u001b[39m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[39m.\u001b[39mpop(filename, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def simulation():\n",
    "    update_target_agent(agent, target_agent)\n",
    "    for ep in range(n_episodes):\n",
    "        episode_steps = episode()\n",
    "        logger.save_episode(episode_steps)\n",
    "        plotter.plot_last_episode()\n",
    "\n",
    "        test_episode_steps = test_episode()\n",
    "        testLogger.save_episode(test_episode_steps)\n",
    "        testPlotter.plot_last_episode()\n",
    "        print('Episode', ep,' finished!')\n",
    "\n",
    "plotter.clear_plots('logs')\n",
    "testPlotter.clear_plots('test_logs')\n",
    "simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
